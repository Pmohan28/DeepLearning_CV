# -*- coding: utf-8 -*-
"""shakespeare_text_generation_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UEayU0kQ6g3LEqB20pzbezYA-uEnxZj
"""



"""# Text generation using LSTMs

This project demonstrates how to generate text using a character-based LSTM. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data ("Shakespear"), train a model to predict the next character in the sequence ("e"). Longer sequences of text can be generated by calling the model repeatedly.

## Setup

**Get the google thing done**
"""

from google.colab import drive

drive.mount('/content/gdrive', force_remount= True)
root_path = 'gdrive/My Drive/'

import os
os.chdir('/content/gdrive/My Drive/NLP/shakespeare_data')

"""### Import Keras and other libraries"""

import glob

from sklearn.utils import shuffle
import numpy as np

from keras.preprocessing.text import Tokenizer
from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional
from keras.optimizers import Adam
from keras import backend

"""### Load the Shakespeare dataset

Run this code on your own data.
"""

# shakelist = glob.glob("./data/*.txt")

shakelist = glob.glob('/content/gdrive/My Drive/NLP/shakespeare_data/*.txt')

shakelist

"""### Read the data

Reading every file
"""

codetext = []
bookranges = []
for shakefile in shakelist:
    shaketext = open(shakefile, "r")
    start = len(codetext)
    codetext.append(shaketext.read())
    end = len(codetext)
    bookranges.append({"start": start, "end": end, "name": shakefile.rsplit("/", 1)[-1]})
    shaketext.close()

"""## Process the text"""

tokenizer = Tokenizer(lower=True, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
tokenizer.fit_on_texts(codetext)

"""### Vectorize the text

Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping words to numbers, and another for numbers to words.
"""

word_idx = tokenizer.word_index
idx_word = tokenizer.index_word

"""Get the word counts"""

word_counts = tokenizer.word_counts
num_words = len(word_idx) + 1

"""Convert text to sequence of numbers"""

sequences = tokenizer.texts_to_sequences(codetext)

"""### Generate Features and Labels"""

features = []
labels = []

training_length = 50
# Iterate through the sequences of tokens
for seq in sequences:
    # Create multiple training examples from each sequence
    for i in range(training_length, training_length+300):
        # Extract the features and label
        extract = seq[i - training_length: i - training_length + 20]

        # Set the features and label
        features.append(extract[:-1])
        labels.append(extract[-1])

"""### The prediction task

Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.

Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?

### Generate training and testing data
"""

from sklearn.utils import shuffle
import numpy as np

features, labels = shuffle(features, labels, random_state=1)

# Decide on number of samples for training
train_end = int(0.7 * len(labels))

train_features = np.array(features[:train_end])
valid_features = np.array(features[train_end:])

train_labels = labels[:train_end]
valid_labels = labels[train_end:]

# Convert to arrays
X_train, X_valid = np.array(train_features), np.array(valid_features)

# Using int8 for memory savings
y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)
y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)

# One hot encoding of labels
for example_index, word_index in enumerate(train_labels):
    y_train[example_index, word_index] = 1

for example_index, word_index in enumerate(valid_labels):
    y_valid[example_index, word_index] = 1

"""This is just to check the features and labels"""

for i, sequence in enumerate(X_train[:2]):
    text = []
#     print(i, sequence)
    for idx in sequence:
        text.append(idx_word[idx])
        
    print('Features: ' + ' '.join(text)+'\n')
    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\n')

"""## Build The Model

Use `keras.Sequential` to define the model. For this simple example three layers are used to define our model:

* `keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;
* `keras.layers.LSTM`: A type of RNN with size `units=rnn_units` (You can also use a GRU layer here.)
* `keras.layers.Dense`: The output layer, with `vocab_size` outputs.
"""

model_1 = Sequential()

# Embedding layer
model_1.add(
    Embedding(
        input_dim=len(word_idx) + 1,
        output_dim=100,
        weights=None,
        trainable=True))

# Recurrent layer
model_1.add(
    LSTM(
        64, return_sequences=False, dropout=0.1,
        recurrent_dropout=0.1))

# Fully connected layer
model_1.add(Dense(64, activation='relu'))

# Dropout for regularization
model_1.add(Dropout(0.5))

# Output layer
model_1.add(Dense(num_words, activation='softmax'))

# Compile the model
model_1.compile(
    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_1.summary()

h_1 = model_1.fit(X_train, y_train, validation_split =0.20,epochs = 50, batch_size = 50, 
          verbose = 1)## Train the model





model_2 = Sequential()

# Embedding layer
model_2.add(
    Embedding(
        input_dim=len(word_idx) + 1,
        output_dim=100,
        weights=None,
        trainable=True))

# Recurrent layer
model_2.add(
    LSTM(
        64))

# Fully connected layer
model_2.add(Dense(64, activation='relu'))

# Dropout for regularization
model_2.add(Dropout(0.5))

# Output layer
model_2.add(Dense(num_words, activation='softmax'))

# Compile the model
model_2.compile(
    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_2.summary()

h = model.fit(X_train, y_train, epochs = 50, batch_size = 50, 
          verbose = 1)## Train the model

"""For each word the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next word.

## Train the model
"""

h = model.fit(X_train, y_train, epochs = 50, batch_size = 50, 
          verbose = 1)## Train the model

"""### Save Model"""

# save the model to file
model.save('./data/model_1000epochs.h5')

pwd

"""### Load Model"""

# load the model
model = load_model('./data/model_1000epochs.h5')

"""### Note: After loading the model run  model.fit()  to continue training form there."""

model.fit(X_train, y_train, batch_size=50, epochs=500)

"""## Evaluation"""

print(model.evaluate(X_train, y_train, batch_size = 20))
print('\nModel Performance: Log Loss and Accuracy on validation data')
print(model.evaluate(X_valid, y_valid, batch_size = 20))

"""## Generate text"""

seed_length=50
new_words=50
diversity=1
n_gen=1

import random

# Choose a random sequence
seq = random.choice(sequences)

# print seq

# Choose a random starting point
seed_idx = random.randint(0, len(seq) - seed_length - 10)
# Ending index for seed
end_idx = seed_idx + seed_length

gen_list = []

for n in range(n_gen):
    # Extract the seed sequence
    seed = seq[seed_idx:end_idx]
    original_sequence = [idx_word[i] for i in seed]
    generated = seed[:] + ['#']

    # Find the actual entire sequence
    actual = generated[:] + seq[end_idx:end_idx + new_words]
        
    # Keep adding new words
    for i in range(new_words):

        # Make a prediction from the seed
        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)

        # Diversify
        preds = np.log(preds) / diversity
        exp_preds = np.exp(preds)

        # Softmax
        preds = exp_preds / sum(exp_preds)

        # Choose the next word
        probas = np.random.multinomial(1, preds, 1)[0]

        next_idx = np.argmax(probas)

        # New seed adds on old word
        #             seed = seed[1:] + [next_idx]
        seed += [next_idx]
        generated.append(next_idx)
    # Showing generated and actual abstract
    n = []

    for i in generated:
        n.append(idx_word.get(i, '< --- >'))

    gen_list.append(n)

a = []

for i in actual:
    a.append(idx_word.get(i, '< --- >'))

a = a[seed_length:]

gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]

print('Original Sequence: \n'+' '.join(original_sequence))
print("\n")
# print(gen_list)
print('Generated Sequence: \n'+' '.join(gen_list[0][1:]))
# print(a)

